#!/usr/bin/env python3
"""
Module 1: News Aggregator
–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import logging
from typing import List, Dict, Optional
import time
import random

from config import RSS_SOURCES, WEBSITE_SOURCES, DAYS_BACK

logger = logging.getLogger(__name__)

class NewsGatherer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def gather_news(self) -> List[Dict]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–±–æ—Ä–∞ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        all_news = []
        
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            logger.info("üì° –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
            rss_news = self._gather_rss_news()
            all_news.extend(rss_news)
            
            # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤
            logger.info("üåê –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤...")
            website_news = self._gather_website_news()
            all_news.extend(website_news)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–µ
            cutoff_date = datetime.now() - timedelta(days=DAYS_BACK)
            filtered_news = []
            
            for news in all_news:
                if news.get('date') and news['date'] >= cutoff_date:
                    filtered_news.append(news)
                elif not news.get('date'):  # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –≤–∫–ª—é—á–∞–µ–º
                    filtered_news.append(news)
            
            logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(filtered_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {DAYS_BACK} –¥–Ω–µ–π")
            return filtered_news
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –Ω–æ–≤–æ—Å—Ç–µ–π: {str(e)}")
            return []
    
    def _gather_rss_news(self) -> List[Dict]:
        """–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        rss_news = []
        
        for source_name, rss_url in RSS_SOURCES.items():
            try:
                logger.info(f"üì° –û–±—Ä–∞–±–æ—Ç–∫–∞ RSS: {source_name}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                time.sleep(random.uniform(1, 3))
                
                feed = feedparser.parse(rss_url)
                
                if feed.bozo:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã —Å RSS {source_name}: {feed.bozo_exception}")
                    continue
                
                for entry in feed.entries:
                    try:
                        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
                        date = self._parse_date(entry.get('published', ''))
                        
                        news_item = {
                            'title': entry.get('title', ''),
                            'description': entry.get('summary', ''),
                            'link': entry.get('link', ''),
                            'date': date,
                            'source': source_name,
                            'source_type': 'rss'
                        }
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
                        if news_item['title'] and news_item['link']:
                            rss_news.append(news_item)
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS –∑–∞–ø–∏—Å–∏ {source_name}: {str(e)}")
                        continue
                        
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ RSS {source_name}: {str(e)}")
                continue
        
        logger.info(f"üì° –°–æ–±—Ä–∞–Ω–æ {len(rss_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        return rss_news
    
    def _gather_website_news(self) -> List[Dict]:
        """–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤"""
        website_news = []
        
        for website_url in WEBSITE_SOURCES:
            try:
                logger.info(f"üåê –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–π—Ç–∞: {website_url}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É
                time.sleep(random.uniform(2, 5))
                
                response = self.session.get(website_url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–±–∞–∑–æ–≤–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
                news_links = self._extract_news_links(soup, website_url)
                
                for link in news_links[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    try:
                        news_item = self._extract_news_from_page(link)
                        if news_item:
                            website_news.append(news_item)
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {link}: {str(e)}")
                        continue
                        
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–∞–π—Ç–∞ {website_url}: {str(e)}")
                continue
        
        logger.info(f"üåê –°–æ–±—Ä–∞–Ω–æ {len(website_news)} –Ω–æ–≤–æ—Å—Ç–µ–π —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤")
        return website_news
    
    def _extract_news_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        news_links = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            text = link.get_text(strip=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ —ç—Ç–æ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å
            if self._looks_like_news_link(href, text):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                if href.startswith('/'):
                    full_url = base_url.rstrip('/') + href
                elif href.startswith('http'):
                    full_url = href
                else:
                    full_url = base_url.rstrip('/') + '/' + href
                
                news_links.append(full_url)
        
        return list(set(news_links))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _looks_like_news_link(self, href: str, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –ø–æ—Ö–æ–∂–∞ –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å"""
        news_keywords = ['news', 'press', 'release', 'announcement', '–Ω–æ–≤–æ—Å—Ç–∏', '–ø—Ä–µ—Å—Å', '—Ä–µ–ª–∏–∑']
        
        href_lower = href.lower()
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Å—Å—ã–ª–∫–µ –∏ —Ç–µ–∫—Å—Ç–µ
        for keyword in news_keywords:
            if keyword in href_lower or keyword in text_lower:
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ (–Ω–æ–≤–æ—Å—Ç–∏ –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏)
        if len(text) > 20 and len(text) < 200:
            return True
        
        return False
    
    def _extract_news_from_page(self, url: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –Ω–æ–≤–æ—Å—Ç–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = self._extract_title(soup)
            if not title:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = self._extract_description(soup)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
            date = self._extract_date(soup)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ URL
            source = self._extract_source_from_url(url)
            
            return {
                'title': title,
                'description': description,
                'link': url,
                'date': date,
                'source': source,
                'source_type': 'website'
            }
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏ —Å {url}: {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_selectors = [
            'h1',
            '.title',
            '.headline',
            '.article-title',
            'title'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title and len(title) > 10:
                    return title
        
        return ""
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è
        desc_selectors = [
            '.description',
            '.summary',
            '.excerpt',
            '.article-summary',
            'meta[name="description"]',
            'p'
        ]
        
        for selector in desc_selectors:
            if selector == 'meta[name="description"]':
                element = soup.select_one(selector)
                if element:
                    return element.get('content', '')
            else:
                element = soup.select_one(selector)
                if element:
                    desc = element.get_text(strip=True)
                    if desc and len(desc) > 50:
                        return desc[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        
        return ""
    
    def _extract_date(self, soup: BeautifulSoup) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –¥–∞—Ç—ã
        date_selectors = [
            '.date',
            '.published',
            '.timestamp',
            'time',
            'meta[property="article:published_time"]'
        ]
        
        for selector in date_selectors:
            element = soup.select_one(selector)
            if element:
                if selector == 'meta[property="article:published_time"]':
                    date_str = element.get('content', '')
                else:
                    date_str = element.get_text(strip=True)
                
                if date_str:
                    parsed_date = self._parse_date(date_str)
                    if parsed_date:
                        return parsed_date
        
        return None
    
    def _extract_source_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ URL"""
        from urllib.parse import urlparse
        
        parsed = urlparse(url)
        domain = parsed.netloc
        
        # –£–±–∏—Ä–∞–µ–º www. –∏ –¥–æ–º–µ–Ω –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è
        source = domain.replace('www.', '').split('.')[0]
        
        return source
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
        if not date_str:
            return None
        
        # –°–ø–∏—Å–æ–∫ —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        date_formats = [
            '%Y-%m-%d',
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%dT%H:%M:%SZ',
            '%d.%m.%Y',
            '%d/%m/%Y',
            '%B %d, %Y',
            '%b %d, %Y'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        return None
